{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlucbw0RAtEXnwUgZ6VzUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopireddy99/NLP_assignments/blob/main/Day_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write a python script - **\n",
        "# Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc."
      ],
      "metadata": {
        "id": "ExeZ7sZX_8sK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASrpw630-80p",
        "outputId": "04384baf-d7f7-4536-e7ec-78ac7f944c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Natural language processing (NLP) is a subfield of artificial intelligence (AI) concerned with the interactions between computers and human (natural) languages, and, in particular, concerns itself with programming computers to fruitfully process large natural language data.\n",
            "\n",
            "\n",
            "Tokenized Sentences:\n",
            "['\\nNatural language processing (NLP) is a subfield of artificial intelligence (AI) concerned with the interactions between computers and human (natural) languages, and, in particular, concerns itself with programming computers to fruitfully process large natural language data.']\n",
            "\n",
            "Tokenized Words:\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerns', 'itself', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'data', '.']\n",
            "\n",
            "Filtered Words (No Stopwords):\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'concerned', 'interactions', 'computers', 'human', '(', 'natural', ')', 'languages', ',', ',', 'particular', ',', 'concerns', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'data', '.']\n",
            "\n",
            "Stemmed Words:\n",
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'artifici', 'intellig', '(', 'ai', ')', 'concern', 'interact', 'comput', 'human', '(', 'natur', ')', 'languag', ',', ',', 'particular', ',', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'data', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'concerned', 'interaction', 'computer', 'human', '(', 'natural', ')', 'language', ',', ',', 'particular', ',', 'concern', 'programming', 'computer', 'fruitfully', 'process', 'large', 'natural', 'language', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text (no external file)\n",
        "sample_text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) concerned with the interactions between computers and human (natural) languages, and, in particular, concerns itself with programming computers to fruitfully process large natural language data.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization (split text into sentences and words)\n",
        "sentences = sent_tokenize(sample_text)\n",
        "words = word_tokenize(sample_text)\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Output the results\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nTokenized Sentences:\")\n",
        "print(sentences)\n",
        "print(\"\\nTokenized Words:\")\n",
        "print(words)\n",
        "print(\"\\nFiltered Words (No Stopwords):\")\n",
        "print(filtered_words)\n",
        "print(\"\\nStemmed Words:\")\n",
        "print(stemmed_words)\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gpc1K8PY_2bh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}